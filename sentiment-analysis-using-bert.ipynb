{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":384451,"sourceType":"datasetVersion","datasetId":169237}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yosefibrahim/sentiment-analysis-using-bert?scriptVersionId=168740434\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:21:35.583137Z","iopub.execute_input":"2024-03-22T15:21:35.583463Z","iopub.status.idle":"2024-03-22T15:21:49.584178Z","shell.execute_reply.started":"2024-03-22T15:21:35.583431Z","shell.execute_reply":"2024-03-22T15:21:49.582783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport gc\nimport random\nfrom nltk.corpus import stopwords\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport transformers\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:48:35.031349Z","iopub.execute_input":"2024-03-22T15:48:35.03234Z","iopub.status.idle":"2024-03-22T15:48:46.449652Z","shell.execute_reply.started":"2024-03-22T15:48:35.032304Z","shell.execute_reply":"2024-03-22T15:48:46.448701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/yelp-review-polarity/yelp_review_polarity_csv/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:49:53.726515Z","iopub.execute_input":"2024-03-22T15:49:53.727322Z","iopub.status.idle":"2024-03-22T15:49:57.438576Z","shell.execute_reply.started":"2024-03-22T15:49:53.727291Z","shell.execute_reply":"2024-03-22T15:49:57.437779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['class','text']","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:49:57.440311Z","iopub.execute_input":"2024-03-22T15:49:57.440668Z","iopub.status.idle":"2024-03-22T15:49:57.445537Z","shell.execute_reply.started":"2024-03-22T15:49:57.440636Z","shell.execute_reply":"2024-03-22T15:49:57.444597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df[:10000]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:49:57.447551Z","iopub.execute_input":"2024-03-22T15:49:57.447825Z","iopub.status.idle":"2024-03-22T15:49:57.456156Z","shell.execute_reply.started":"2024-03-22T15:49:57.447803Z","shell.execute_reply":"2024-03-22T15:49:57.455313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"class\"]=df[\"class\"].apply(lambda x: 0 if x == 2 else x)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:49:57.457901Z","iopub.execute_input":"2024-03-22T15:49:57.458182Z","iopub.status.idle":"2024-03-22T15:49:57.475743Z","shell.execute_reply.started":"2024-03-22T15:49:57.45816Z","shell.execute_reply":"2024-03-22T15:49:57.474951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:50:05.001291Z","iopub.execute_input":"2024-03-22T15:50:05.001926Z","iopub.status.idle":"2024-03-22T15:50:05.015492Z","shell.execute_reply.started":"2024-03-22T15:50:05.001897Z","shell.execute_reply":"2024-03-22T15:50:05.014525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:50:08.019802Z","iopub.execute_input":"2024-03-22T15:50:08.02019Z","iopub.status.idle":"2024-03-22T15:50:08.082614Z","shell.execute_reply.started":"2024-03-22T15:50:08.020159Z","shell.execute_reply":"2024-03-22T15:50:08.08152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sw = stopwords.words('english')\n\ndef clean_text(text):\n    \n    text = text.lower()\n    \n    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    #text = re.sub(r\"http\", \"\",text)\n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:50:41.499536Z","iopub.execute_input":"2024-03-22T15:50:41.499901Z","iopub.status.idle":"2024-03-22T15:50:41.514514Z","shell.execute_reply.started":"2024-03-22T15:50:41.499873Z","shell.execute_reply":"2024-03-22T15:50:41.513728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:50:53.72522Z","iopub.execute_input":"2024-03-22T15:50:53.726019Z","iopub.status.idle":"2024-03-22T15:50:57.360467Z","shell.execute_reply.started":"2024-03-22T15:50:53.725986Z","shell.execute_reply":"2024-03-22T15:50:57.359695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = df.text.values\nlabels = df['class'].values","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:51:21.777822Z","iopub.execute_input":"2024-03-22T15:51:21.778648Z","iopub.status.idle":"2024-03-22T15:51:21.783237Z","shell.execute_reply.started":"2024-03-22T15:51:21.778607Z","shell.execute_reply":"2024-03-22T15:51:21.782252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:51:31.186308Z","iopub.execute_input":"2024-03-22T15:51:31.186678Z","iopub.status.idle":"2024-03-22T15:51:32.193627Z","shell.execute_reply.started":"2024-03-22T15:51:31.18665Z","shell.execute_reply":"2024-03-22T15:51:32.192673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(' Original: ', tweets[0])\nprint()\n# Print the sentence split into tokens.\nprint('Tokenized: ', tokenizer.tokenize(tweets[0]))\nprint()\n# Print the sentence mapped to token ids.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:52:01.55032Z","iopub.execute_input":"2024-03-22T15:52:01.551074Z","iopub.status.idle":"2024-03-22T15:52:01.560133Z","shell.execute_reply.started":"2024-03-22T15:52:01.551022Z","shell.execute_reply":"2024-03-22T15:52:01.559175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\n\n# For every sentence...\nfor sent in tweets:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:52:26.941527Z","iopub.execute_input":"2024-03-22T15:52:26.942173Z","iopub.status.idle":"2024-03-22T15:52:55.884561Z","shell.execute_reply.started":"2024-03-22T15:52:26.942142Z","shell.execute_reply":"2024-03-22T15:52:55.883478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 128","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:56:21.5779Z","iopub.execute_input":"2024-03-22T15:56:21.578752Z","iopub.status.idle":"2024-03-22T15:56:21.58275Z","shell.execute_reply.started":"2024-03-22T15:56:21.578709Z","shell.execute_reply":"2024-03-22T15:56:21.5818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n# For every tweet...\nfor tweet in tweets:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = max_len,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', tweets[0])\nprint('Token IDs:', input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:56:21.993617Z","iopub.execute_input":"2024-03-22T15:56:21.994451Z","iopub.status.idle":"2024-03-22T15:56:52.485202Z","shell.execute_reply.started":"2024-03-22T15:56:21.994418Z","shell.execute_reply":"2024-03-22T15:56:52.48424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 90-10 train-validation split.\n\n# Calculate the number of samples to include in each set.\ntrain_size = int(0.8 * len(dataset))\n#val_size = int(0.2 * len(dataset))\nval_size = len(dataset)  - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:57:00.891227Z","iopub.execute_input":"2024-03-22T15:57:00.891936Z","iopub.status.idle":"2024-03-22T15:57:00.904546Z","shell.execute_reply.started":"2024-03-22T15:57:00.891906Z","shell.execute_reply":"2024-03-22T15:57:00.903655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:57:26.91363Z","iopub.execute_input":"2024-03-22T15:57:26.914088Z","iopub.status.idle":"2024-03-22T15:57:26.919976Z","shell.execute_reply.started":"2024-03-22T15:57:26.914051Z","shell.execute_reply":"2024-03-22T15:57:26.919039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# if device == \"cuda:0\":\n# # Tell pytorch to run this model on the GPU.\n#     model = model.cuda()\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:57:40.009756Z","iopub.execute_input":"2024-03-22T15:57:40.01059Z","iopub.status.idle":"2024-03-22T15:57:43.522477Z","shell.execute_reply.started":"2024-03-22T15:57:40.010558Z","shell.execute_reply":"2024-03-22T15:57:43.521583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:58:00.876183Z","iopub.execute_input":"2024-03-22T15:58:00.877281Z","iopub.status.idle":"2024-03-22T15:58:00.889659Z","shell.execute_reply.started":"2024-03-22T15:58:00.877242Z","shell.execute_reply":"2024-03-22T15:58:00.888518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:58:09.285531Z","iopub.execute_input":"2024-03-22T15:58:09.285897Z","iopub.status.idle":"2024-03-22T15:58:09.291322Z","shell.execute_reply.started":"2024-03-22T15:58:09.28587Z","shell.execute_reply":"2024-03-22T15:58:09.290281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:58:18.29806Z","iopub.execute_input":"2024-03-22T15:58:18.298783Z","iopub.status.idle":"2024-03-22T15:58:18.303874Z","shell.execute_reply.started":"2024-03-22T15:58:18.29875Z","shell.execute_reply":"2024-03-22T15:58:18.302854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:58:26.10669Z","iopub.execute_input":"2024-03-22T15:58:26.1073Z","iopub.status.idle":"2024-03-22T15:58:26.11257Z","shell.execute_reply.started":"2024-03-22T15:58:26.107267Z","shell.execute_reply":"2024-03-22T15:58:26.111528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        optimizer.zero_grad()\n        output = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)        \n        loss = output.loss\n        total_train_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    total_eval_accuracy = 0\n    best_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n        loss = output.loss\n        total_eval_loss += loss.item()\n        # Move logits and labels to CPU if we are using GPU\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    if avg_val_accuracy > best_eval_accuracy:\n        torch.save(model, 'bert_model')\n        best_eval_accuracy = avg_val_accuracy\n    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    #print(\"  Validation took: {:}\".format(validation_time))\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:00:06.51192Z","iopub.execute_input":"2024-03-22T16:00:06.512492Z","iopub.status.idle":"2024-03-22T16:12:22.999762Z","shell.execute_reply.started":"2024-03-22T16:00:06.512461Z","shell.execute_reply":"2024-03-22T16:12:22.998367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('bert_model')","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:46:08.654981Z","iopub.execute_input":"2024-03-22T16:46:08.655876Z","iopub.status.idle":"2024-03-22T16:46:08.987288Z","shell.execute_reply.started":"2024-03-22T16:46:08.655843Z","shell.execute_reply":"2024-03-22T16:46:08.986435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/yelp-review-polarity/yelp_review_polarity_csv/test.csv')\ndf_test.columns = ['class','text']\ndf_test[\"class\"]=df_test[\"class\"].apply(lambda x: 0 if x == 2 else x)\ndf_test=df_test[:2000]\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:55:18.736813Z","iopub.execute_input":"2024-03-22T16:55:18.737166Z","iopub.status.idle":"2024-03-22T16:55:18.994095Z","shell.execute_reply.started":"2024-03-22T16:55:18.737142Z","shell.execute_reply":"2024-03-22T16:55:18.993184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text'] = df_test['text'].apply(lambda x:clean_text(x))\ntest_tweets = df_test['text'].values","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:55:24.434803Z","iopub.execute_input":"2024-03-22T16:55:24.435203Z","iopub.status.idle":"2024-03-22T16:55:25.157202Z","shell.execute_reply.started":"2024-03-22T16:55:24.435171Z","shell.execute_reply":"2024-03-22T16:55:25.156424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input_ids = []\ntest_attention_masks = []\nfor tweet in test_tweets:\n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                     \n                        add_special_tokens = True, \n                        max_length = max_len,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,\n                        return_tensors = 'pt',\n                   )\n    test_input_ids.append(encoded_dict['input_ids'])\n    test_attention_masks.append(encoded_dict['attention_mask'])\ntest_input_ids = torch.cat(test_input_ids, dim=0)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:55:27.303324Z","iopub.execute_input":"2024-03-22T16:55:27.303679Z","iopub.status.idle":"2024-03-22T16:55:33.300903Z","shell.execute_reply.started":"2024-03-22T16:55:27.303651Z","shell.execute_reply":"2024-03-22T16:55:33.300083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TensorDataset(test_input_ids, test_attention_masks)\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:55:38.822764Z","iopub.execute_input":"2024-03-22T16:55:38.823478Z","iopub.status.idle":"2024-03-22T16:55:38.832445Z","shell.execute_reply.started":"2024-03-22T16:55:38.823445Z","shell.execute_reply":"2024-03-22T16:55:38.831579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = output.logits\n            logits = logits.detach().cpu().numpy()\n            pred_flat = np.argmax(logits, axis=1).flatten()\n            \n            predictions.extend(list(pred_flat))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:55:42.050927Z","iopub.execute_input":"2024-03-22T16:55:42.051312Z","iopub.status.idle":"2024-03-22T16:55:57.650062Z","shell.execute_reply.started":"2024-03-22T16:55:42.05128Z","shell.execute_reply":"2024-03-22T16:55:57.649322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_output = pd.DataFrame()\n#df_output['id'] = df_test['id']\ndf_output['target'] =predictions\ndf_output.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:56:31.266375Z","iopub.execute_input":"2024-03-22T16:56:31.267213Z","iopub.status.idle":"2024-03-22T16:56:31.299009Z","shell.execute_reply.started":"2024-03-22T16:56:31.267176Z","shell.execute_reply":"2024-03-22T16:56:31.298329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_output.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T16:57:01.91791Z","iopub.execute_input":"2024-03-22T16:57:01.918524Z","iopub.status.idle":"2024-03-22T16:57:01.926554Z","shell.execute_reply.started":"2024-03-22T16:57:01.918493Z","shell.execute_reply":"2024-03-22T16:57:01.925581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.text[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:07:55.440526Z","iopub.execute_input":"2024-03-22T17:07:55.440919Z","iopub.status.idle":"2024-03-22T17:07:55.447198Z","shell.execute_reply.started":"2024-03-22T17:07:55.440889Z","shell.execute_reply":"2024-03-22T17:07:55.446258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text='last summer appointment get new tires wait super long time also went week\\\nfix minor problem tire put fixed free, next morning issue called complain, manager\\\neven apologize frustrated never going back seem overpriced,'","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:27:39.825047Z","iopub.execute_input":"2024-03-22T17:27:39.825416Z","iopub.status.idle":"2024-03-22T17:27:39.829885Z","shell.execute_reply.started":"2024-03-22T17:27:39.825388Z","shell.execute_reply":"2024-03-22T17:27:39.828869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dict = tokenizer.encode_plus(\n                        text,                     \n                        add_special_tokens = True, \n                        max_length = max_len,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,\n                        return_tensors = 'pt',\n                   )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:27:40.431061Z","iopub.execute_input":"2024-03-22T17:27:40.431375Z","iopub.status.idle":"2024-03-22T17:27:40.438999Z","shell.execute_reply.started":"2024-03-22T17:27:40.431348Z","shell.execute_reply":"2024-03-22T17:27:40.438057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input_ids=[encoded_dict['input_ids']]\ntest_attention_masks=[encoded_dict['attention_mask']]\ntest_input_ids = torch.cat(test_input_ids, dim=0).to(device)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:27:41.042006Z","iopub.execute_input":"2024-03-22T17:27:41.042334Z","iopub.status.idle":"2024-03-22T17:27:41.048783Z","shell.execute_reply.started":"2024-03-22T17:27:41.042309Z","shell.execute_reply":"2024-03-22T17:27:41.04794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith torch.no_grad():        \n    output= model(test_input_ids, \n                           token_type_ids=None, \n                           attention_mask=test_attention_masks)\n    logits = output.logits\n    logits = logits.detach().cpu().numpy()\n    pred_flat = np.argmax(logits, axis=1).flatten()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:28:00.294796Z","iopub.execute_input":"2024-03-22T17:28:00.295276Z","iopub.status.idle":"2024-03-22T17:28:00.319023Z","shell.execute_reply.started":"2024-03-22T17:28:00.295241Z","shell.execute_reply":"2024-03-22T17:28:00.318079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_flat[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:28:01.188424Z","iopub.execute_input":"2024-03-22T17:28:01.188768Z","iopub.status.idle":"2024-03-22T17:28:01.194978Z","shell.execute_reply.started":"2024-03-22T17:28:01.188741Z","shell.execute_reply":"2024-03-22T17:28:01.194052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def product(text):\n    text=clean_text(text)\n    encoded_dict = tokenizer.encode_plus(\n                        text,                     \n                        add_special_tokens = True, \n                        max_length = max_len,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,\n                        return_tensors = 'pt',\n                   )\n    test_input_ids=[encoded_dict['input_ids']]\n    test_attention_masks=[encoded_dict['attention_mask']]\n    test_input_ids = torch.cat(test_input_ids, dim=0).to(device)\n    test_attention_masks = torch.cat(test_attention_masks, dim=0).to(device)\n    with torch.no_grad():        \n        output= model(test_input_ids, \n                               token_type_ids=None, \n                               attention_mask=test_attention_masks)\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        pred_flat = np.argmax(logits, axis=1).flatten()\n    if pred_flat==0:\n        print(\"negative\")\n    else:\n        print(\"Positive\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:29:14.849898Z","iopub.execute_input":"2024-03-22T17:29:14.850298Z","iopub.status.idle":"2024-03-22T17:29:14.858424Z","shell.execute_reply.started":"2024-03-22T17:29:14.850268Z","shell.execute_reply":"2024-03-22T17:29:14.857465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product(df_test.text[2])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:29:26.74146Z","iopub.execute_input":"2024-03-22T17:29:26.742271Z","iopub.status.idle":"2024-03-22T17:29:26.765961Z","shell.execute_reply.started":"2024-03-22T17:29:26.742239Z","shell.execute_reply":"2024-03-22T17:29:26.765096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## second model","metadata":{}},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:04:10.614148Z","iopub.execute_input":"2024-03-22T18:04:10.614535Z","iopub.status.idle":"2024-03-22T18:04:10.785538Z","shell.execute_reply.started":"2024-03-22T18:04:10.614507Z","shell.execute_reply":"2024-03-22T18:04:10.784579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n# For every tweet...\nfor tweet in tweets:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = max_len,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', tweets[0])\nprint('Token IDs:', input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:04:11.754669Z","iopub.execute_input":"2024-03-22T18:04:11.755763Z","iopub.status.idle":"2024-03-22T18:04:42.500237Z","shell.execute_reply.started":"2024-03-22T18:04:11.75572Z","shell.execute_reply":"2024-03-22T18:04:42.499351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 90-10 train-validation split.\n\n# Calculate the number of samples to include in each set.\ntrain_size = int(0.8 * len(dataset))\n#val_size = int(0.2 * len(dataset))\nval_size = len(dataset)  - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nval_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:24:38.431588Z","iopub.execute_input":"2024-03-22T18:24:38.431974Z","iopub.status.idle":"2024-03-22T18:24:38.437648Z","shell.execute_reply.started":"2024-03-22T18:24:38.431947Z","shell.execute_reply":"2024-03-22T18:24:38.436697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# if device == \"cuda:0\":\n# # Tell pytorch to run this model on the GPU.\n#     model = model.cuda()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:05:47.344719Z","iopub.execute_input":"2024-03-22T18:05:47.345394Z","iopub.status.idle":"2024-03-22T18:05:47.787337Z","shell.execute_reply.started":"2024-03-22T18:05:47.345362Z","shell.execute_reply":"2024-03-22T18:05:47.786562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport torch.nn as nn\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False, version=\"base\"):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in = 768\n        H, D_out = 50, 2\n\n        # Instantiate BERT model\n        self.bert =BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2,output_attentions = False, output_hidden_states = False, )\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:30.958094Z","iopub.execute_input":"2024-03-22T18:46:30.95847Z","iopub.status.idle":"2024-03-22T18:46:30.968528Z","shell.execute_reply.started":"2024-03-22T18:46:30.958443Z","shell.execute_reply":"2024-03-22T18:46:30.967631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:31.566845Z","iopub.execute_input":"2024-03-22T18:46:31.567174Z","iopub.status.idle":"2024-03-22T18:46:31.574014Z","shell.execute_reply.started":"2024-03-22T18:46:31.567148Z","shell.execute_reply":"2024-03-22T18:46:31.573184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_classifier = BertClassifier(freeze_bert=False)\n# Tell PyTorch to run the model on GPU\nbert_classifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:32.596429Z","iopub.execute_input":"2024-03-22T18:46:32.596751Z","iopub.status.idle":"2024-03-22T18:46:32.982172Z","shell.execute_reply.started":"2024-03-22T18:46:32.596725Z","shell.execute_reply":"2024-03-22T18:46:32.981281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(bert_classifier.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:35.904738Z","iopub.execute_input":"2024-03-22T18:46:35.905632Z","iopub.status.idle":"2024-03-22T18:46:35.913898Z","shell.execute_reply.started":"2024-03-22T18:46:35.905588Z","shell.execute_reply":"2024-03-22T18:46:35.913099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:36.283719Z","iopub.execute_input":"2024-03-22T18:46:36.284023Z","iopub.status.idle":"2024-03-22T18:46:36.289039Z","shell.execute_reply.started":"2024-03-22T18:46:36.283998Z","shell.execute_reply":"2024-03-22T18:46:36.288058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:37.300585Z","iopub.execute_input":"2024-03-22T18:46:37.30137Z","iopub.status.idle":"2024-03-22T18:46:37.306324Z","shell.execute_reply.started":"2024-03-22T18:46:37.301342Z","shell.execute_reply":"2024-03-22T18:46:37.305284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:38.542347Z","iopub.execute_input":"2024-03-22T18:46:38.54307Z","iopub.status.idle":"2024-03-22T18:46:38.547909Z","shell.execute_reply.started":"2024-03-22T18:46:38.543017Z","shell.execute_reply":"2024-03-22T18:46:38.546874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    total_train_loss = 0\n    bert_classifier.train()\n    for step, batch in enumerate(train_dataloader):\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        optimizer.zero_grad()\n        output = bert_classifier(b_input_ids,b_input_mask)        \n        loss = output.loss\n        total_train_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(bert_classifier.parameters(), 1.0)\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    bert_classifier.eval()\n    # Tracking variables \n    total_eval_accuracy = 0\n    best_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n            output= bert_classifier(b_input_ids, \n                                   attention_mask=b_input_mask)\n        loss = output.loss\n        total_eval_loss += loss.item()\n        # Move logits and labels to CPU if we are using GPU\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    if avg_val_accuracy > best_eval_accuracy:\n        torch.save(bert_classifier, 'bert_model')\n        best_eval_accuracy = avg_val_accuracy\n    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    #print(\"  Validation took: {:}\".format(validation_time))\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:46:54.517458Z","iopub.execute_input":"2024-03-22T18:46:54.518186Z","iopub.status.idle":"2024-03-22T18:46:54.665358Z","shell.execute_reply.started":"2024-03-22T18:46:54.518156Z","shell.execute_reply":"2024-03-22T18:46:54.664192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}